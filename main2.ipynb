{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7592210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_19176\\1938280762.py:18: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  'dns': pd.read_csv('labeled_dataset_DNS_Spoofing.csv'),\n",
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_19176\\1938280762.py:19: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  'mitm': pd.read_csv('labeled_dataset_MITM_ArpSpoofing.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883525, 135)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load and combine datasets (as before)\n",
    "datasets = {\n",
    "    'benign': pd.read_csv('labeled_dataset_benign.csv'),\n",
    "    'dns': pd.read_csv('labeled_dataset_DNS_Spoofing.csv'),\n",
    "    'mitm': pd.read_csv('labeled_dataset_MITM_ArpSpoofing.csv')\n",
    "}\n",
    "combined_df = pd.concat([datasets['benign'], datasets['dns'], datasets['mitm']], ignore_index=True)\n",
    "combined_df = combined_df.drop('dst_mac', axis=1)\n",
    "\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a89a1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "New shape after removing duplicates: (432555, 135)\n",
      "Categorical features: ['src_mac', 'src_ip', 'dst_ip', 'port_class_dst', 'l4_tcp', 'l4_udp', 'ttl', 'handshake_version', 'handshake_ciphersuites', 'tls_server', 'http_request_method', 'http_host', 'http_response_code', 'user_agent', 'dns_server', 'dns_query_type', 'dns_len_ans', 'device_mac', 'eth_src_oui', 'eth_dst_oui', 'highest_layer', 'http_uri', 'http_content_len', 'http_content_type', 'icmp_type', 'icmp_checksum_status', 'icmp_data_size', 'Label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_19176\\2122749699.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['All_Labels'] = combined_df.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def is_hex_mac(mac):\n",
    "    if pd.isna(mac) or mac == 'unknown':\n",
    "        return False\n",
    "    pattern = r'^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$'\n",
    "    return bool(re.match(pattern, str(mac)))\n",
    "\n",
    "\n",
    "# Remove rows where src_mac is a real MAC address\n",
    "combined_df = combined_df[~combined_df['src_mac'].apply(is_hex_mac)]\n",
    "\n",
    "# Drop duplicates\n",
    "print(f\"Number of duplicate rows: {combined_df.duplicated().sum()}\")\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "print(f\"New shape after removing duplicates: {combined_df.shape}\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = [col for col in combined_df.columns if combined_df[col].nunique() < 10 or combined_df[col].dtype == 'object']\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "\n",
    "# Convert all categorical columns to string and fill missing with 'unknown'\n",
    "for col in categorical_cols:\n",
    "    combined_df[col] = combined_df[col].astype(str).fillna('unknown')\n",
    "\n",
    "# Fill missing numerical values with median\n",
    "numerical_cols = combined_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    combined_df[col] = combined_df[col].fillna(combined_df[col].median())\n",
    "\n",
    "# --- Step 2: Split target labels and features ---\n",
    "X = combined_df.drop(['Label', 'src_mac'], axis=1, errors='ignore')\n",
    "y_attack = combined_df['Label']\n",
    "y_device = combined_df['src_mac']\n",
    "\n",
    "# Encode device labels\n",
    "le_device = LabelEncoder()\n",
    "y_device_encoded = le_device.fit_transform(y_device)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# 1. Create a column that holds both relevant labels as a list for each row\n",
    "# This creates a list ['Attack_Name', 'Device_MAC'] for every packet\n",
    "combined_df['All_Labels'] = combined_df.apply(\n",
    "    lambda row: [row['Label'], row['src_mac']], axis=1\n",
    ")\n",
    "\n",
    "# 2. Initialize and fit the binarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_multilabel = mlb.fit_transform(combined_df['All_Labels'])\n",
    "\n",
    "# 3. Store the column names (needed for evaluation later)\n",
    "multilabel_class_names = mlb.classes_\n",
    "\n",
    "\n",
    "# Keep only numeric columns\n",
    "X_numeric = X.select_dtypes(include=['number'])\n",
    "\n",
    "# Normalize features using StandardScaler (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Apply PCA to retain 95% variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_feature_names = [f'PC{i+1}' for i in range(X_reduced.shape[1])]\n",
    "\n",
    "# Convert to DataFrame for ease of use\n",
    "X_reduced_df = pd.DataFrame(X_reduced, columns=pca_feature_names, index=X.index)\n",
    "\n",
    "# Replace the single-label splits with the multi-label split:\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_reduced_df,       # Your PCA features\n",
    "    Y_multilabel,       # Your multi-label target matrix\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ba546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Multi-Label LightGBM ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, hamming_loss, classification_report\n",
    "\n",
    "# --- 1. Define the Base Estimator (LightGBM) ---\n",
    "print(\"--- Training Multi-Label LightGBM ---\")\n",
    "# Apply regularization and weighted loss to the base model\n",
    "lgb_base = lgb.LGBMClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    n_jobs=2,\n",
    "    max_depth=8,              # Regularization: Limit tree depth\n",
    "    reg_alpha=0.2,            # Regularization: L1\n",
    "    # Weighted Loss: Applies balancing to each of the output's binary problems\n",
    "    class_weight='balanced'   \n",
    ")\n",
    "\n",
    "# --- 2. Wrap it with MultiOutputClassifier ---\n",
    "# This trains M LightGBM models (where M is the number of total unique labels)\n",
    "lgb_multioutput = MultiOutputClassifier(lgb_base, n_jobs=-1)\n",
    "\n",
    "# --- 3. Fit the Model ---\n",
    "# This will take longer than a single multi-class model\n",
    "lgb_multioutput.fit(X_train, Y_train)\n",
    "\n",
    "# --- 4. Prediction ---\n",
    "Y_pred_lgbm = lgb_multioutput.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel_model(Y_test, Y_pred, class_names):\n",
    "    \"\"\"Calculates and prints key multi-label metrics.\"\"\"\n",
    "    print(\"\\n======== Evaluation: LightGBM Multi-Output ========\")\n",
    "    \n",
    "    # 1. Subset Accuracy (STRICTEST: Requires *ALL* labels to match exactly)\n",
    "    subset_acc = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "\n",
    "    # 2. Jaccard Score (Average of Intersection over Union)\n",
    "    # This is often the most informative metric for multi-label\n",
    "    jaccard = jaccard_score(Y_test, Y_pred, average='samples')\n",
    "    print(f\"Jaccard Score (Label Similarity): {jaccard:.4f}\")\n",
    "\n",
    "    # 3. Hamming Loss (Fraction of incorrect labels)\n",
    "    # Lower is better; a perfect score is 0.0\n",
    "    hamming = hamming_loss(Y_test, Y_pred)\n",
    "    print(f\"Hamming Loss (Error Rate): {hamming:.4f}\")\n",
    "    \n",
    "    # 4. Detailed Per-Label Metrics (Micro/Macro averages)\n",
    "    print(\"\\n--- Micro-Averaged Metrics (Focus on Total Correctness) ---\")\n",
    "    # Note: Target names should come from the mlb.classes_ variable\n",
    "    print(classification_report(Y_test, Y_pred, \n",
    "                                target_names=class_names, \n",
    "                                zero_division=0, \n",
    "                                output_dict=False))\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_multilabel_model(Y_test, Y_pred_lgbm, multilabel_class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
