{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7592210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_10880\\1938280762.py:18: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  'dns': pd.read_csv('labeled_dataset_DNS_Spoofing.csv'),\n",
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_10880\\1938280762.py:19: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  'mitm': pd.read_csv('labeled_dataset_MITM_ArpSpoofing.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883525, 135)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load and combine datasets (as before)\n",
    "datasets = {\n",
    "    'benign': pd.read_csv('labeled_dataset_benign.csv'),\n",
    "    'dns': pd.read_csv('labeled_dataset_DNS_Spoofing.csv'),\n",
    "    'mitm': pd.read_csv('labeled_dataset_MITM_ArpSpoofing.csv')\n",
    "}\n",
    "combined_df = pd.concat([datasets['benign'], datasets['dns'], datasets['mitm']], ignore_index=True)\n",
    "combined_df = combined_df.drop('dst_mac', axis=1)\n",
    "\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89a1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "New shape after removing duplicates: (432555, 135)\n",
      "Categorical features: ['src_mac', 'src_ip', 'dst_ip', 'port_class_dst', 'l4_tcp', 'l4_udp', 'ttl', 'handshake_version', 'handshake_ciphersuites', 'tls_server', 'http_request_method', 'http_host', 'http_response_code', 'user_agent', 'dns_server', 'dns_query_type', 'dns_len_ans', 'device_mac', 'eth_src_oui', 'eth_dst_oui', 'highest_layer', 'http_uri', 'http_content_len', 'http_content_type', 'icmp_type', 'icmp_checksum_status', 'icmp_data_size', 'Label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_10880\\734139780.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['All_Labels'] = combined_df.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def is_hex_mac(mac):\n",
    "    if pd.isna(mac) or mac == 'unknown':\n",
    "        return False\n",
    "    pattern = r'^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$'\n",
    "    return bool(re.match(pattern, str(mac)))\n",
    "\n",
    "\n",
    "combined_df = combined_df[~combined_df['src_mac'].apply(is_hex_mac)]\n",
    "\n",
    "# Drop duplicates\n",
    "print(f\"Number of duplicate rows: {combined_df.duplicated().sum()}\")\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "print(f\"New shape after removing duplicates: {combined_df.shape}\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = [col for col in combined_df.columns if combined_df[col].nunique() < 10 or combined_df[col].dtype == 'object']\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    combined_df[col] = combined_df[col].astype(str).fillna('unknown')\n",
    "\n",
    "numerical_cols = combined_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    combined_df[col] = combined_df[col].fillna(combined_df[col].median())\n",
    "\n",
    "X = combined_df.drop(['Label', 'src_mac'], axis=1, errors='ignore')\n",
    "y_attack = combined_df['Label']\n",
    "y_device = combined_df['src_mac']\n",
    "\n",
    "# Encode device labels\n",
    "le_device = LabelEncoder()\n",
    "y_device_encoded = le_device.fit_transform(y_device)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "combined_df['All_Labels'] = combined_df.apply(\n",
    "    lambda row: [row['Label'], row['src_mac']], axis=1\n",
    ")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_multilabel = mlb.fit_transform(combined_df['All_Labels'])\n",
    "\n",
    "multilabel_class_names = mlb.classes_\n",
    "\n",
    "\n",
    "X_numeric = X.select_dtypes(include=['number'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled,       \n",
    "    Y_multilabel,       \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20c4ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RFE to select 50 features...\n",
      "Fitting estimator with 107 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:17:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 102 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:17:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 97 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 92 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 87 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 82 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 77 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 72 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 67 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 62 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 57 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 52 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE complete. Using 50 features.\n",
      "Selected features: [0, 1, 2, 3, 6]... (first 5 shown)\n",
      "\n",
      "======================================================================\n",
      "--- Training Multi-Label Random Forest (Binary Relevance) on RFE Features ---\n",
      "======================================================================\n",
      "Fitting individual Random Forest models (one per label)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m     rf_estimator \u001b[38;5;241m=\u001b[39m clone(rf_base) \n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Train the estimator for the current label\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mrf_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_RFE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     fitted_rf_estimators\u001b[38;5;241m.\u001b[39mappend(rf_estimator)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# --- Prediction ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, jaccard_score, classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "xgb_rfe_estimator = XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=5, \n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "TARGET_FEATURES = 50 \n",
    "rfe_selector = RFE(\n",
    "    estimator=xgb_rfe_estimator, \n",
    "    n_features_to_select=TARGET_FEATURES, \n",
    "    step=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "Y_single_label = Y_train[:, 0]\n",
    "try:\n",
    "    multilabel_class_names = mlb.classes_\n",
    "except NameError:\n",
    "    multilabel_class_names = [f\"Label_{i+1}\" for i in range(Y_train.shape[1])]\n",
    "\n",
    "\n",
    "print(f\"Starting RFE to select {TARGET_FEATURES} features...\")\n",
    "rfe_selector.fit(X_train, Y_single_label)\n",
    "\n",
    "selected_features = X_train.columns[rfe_selector.support_]\n",
    "\n",
    "X_train_RFE = X_train[selected_features]\n",
    "X_test_RFE = X_test[selected_features]\n",
    "\n",
    "print(f\"RFE complete. Using {len(selected_features)} features.\")\n",
    "print(f\"Selected features: {selected_features.tolist()[:5]}... (first 5 shown)\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Training Multi-Label Random Forest (Binary Relevance) on RFE Features ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced' \n",
    ")\n",
    "\n",
    "num_outputs = Y_train.shape[1]\n",
    "fitted_rf_estimators = [] \n",
    "\n",
    "print(\"Fitting individual Random Forest models (one per label)...\")\n",
    "\n",
    "for i in range(num_outputs):\n",
    "    y_col = Y_train[:, i]\n",
    "    \n",
    "   \n",
    "    rf_estimator = clone(rf_base) \n",
    "    \n",
    "    rf_estimator.fit(X_train_RFE, y_col)\n",
    "    \n",
    "    fitted_rf_estimators.append(rf_estimator)\n",
    "\n",
    "\n",
    "Y_pred_rf = np.zeros_like(Y_test)\n",
    "\n",
    "for i, estimator in enumerate(fitted_rf_estimators):\n",
    "    Y_pred_rf[:, i] = estimator.predict(X_test_RFE)\n",
    "\n",
    "\n",
    "\n",
    "subset_acc = accuracy_score(Y_test, Y_pred_rf)\n",
    "jaccard = jaccard_score(Y_test, Y_pred_rf, average='samples', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RANDOM FOREST (RFE FEATURES) TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "print(f\"Jaccard Score (Label Similarity): {jaccard:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED MULTI-LABEL CLASSIFICATION REPORT (Per-Label Metrics)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "report = classification_report(\n",
    "    Y_test, \n",
    "    Y_pred_rf, \n",
    "    target_names=multilabel_class_names, \n",
    "    zero_division=0,\n",
    "    output_dict=False\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0db103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.metrics import accuracy_score, jaccard_score, classification_report\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import catboost as cb\n",
    "import lightgbm as lgb \n",
    "\n",
    "\n",
    "Y_single_label = Y_train[:, 0]\n",
    "try:\n",
    "    multilabel_class_names = mlb.classes_\n",
    "except NameError:\n",
    "    multilabel_class_names = [f\"Label_{i+1}\" for i in range(Y_train.shape[1])]\n",
    "\n",
    "\n",
    "TARGET_FEATURES = 50\n",
    "xgb_rfe_estimator = XGBClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1, use_label_encoder=False, eval_metric='logloss')\n",
    "rfe_selector = RFE(estimator=xgb_rfe_estimator, n_features_to_select=TARGET_FEATURES, step=5, verbose=0)\n",
    "\n",
    "# Run RFE\n",
    "print(f\"Starting RFE to select {TARGET_FEATURES} features...\")\n",
    "rfe_selector.fit(X_train, Y_single_label)\n",
    "\n",
    "selected_features = X_train.columns[rfe_selector.support_]\n",
    "X_train_RFE = X_train[selected_features]\n",
    "X_test_RFE = X_test[selected_features]\n",
    "\n",
    "print(f\"RFE complete. Using {len(selected_features)} features.\")\n",
    "\n",
    "model_configs = {\n",
    "    # Traditional Models\n",
    "    'RandomForest': {\n",
    "        'base_estimator': RandomForestClassifier(n_estimators=500, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'base_estimator': LogisticRegression(solver='liblinear', random_state=42, n_jobs=1, class_weight='balanced'),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'LinearSVC': {\n",
    "        'base_estimator': LinearSVC(random_state=42, class_weight='balanced', dual=True),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'base_estimator': AdaBoostClassifier(n_estimators=500, random_state=42),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'base_estimator': lgb.LGBMClassifier(\n",
    "            n_estimators=500, \n",
    "            random_state=42, \n",
    "            n_jobs=-1,\n",
    "            scale_pos_weight=1, \n",
    "            objective='binary' \n",
    "        ),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'base_estimator': cb.CatBoostClassifier(\n",
    "            iterations=500, \n",
    "            random_state=42, \n",
    "            verbose=0, \n",
    "            thread_count=-1,\n",
    "            auto_class_weights='Balanced', \n",
    "            loss_function='Logloss'\n",
    "        ),\n",
    "        'predictions': None\n",
    "    }\n",
    "}\n",
    "\n",
    "num_outputs = Y_train.shape[1]\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Starting Multi-Label Model Comparison (Binary Relevance) ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    \n",
    "    base_estimator = config['base_estimator']\n",
    "    fitted_estimators = [] \n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        y_col = Y_train[:, i]\n",
    "        \n",
    "        pos_count = np.sum(y_col == 1)\n",
    "        neg_count = np.sum(y_col == 0)\n",
    "        weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "        estimator = clone(base_estimator) \n",
    "        \n",
    "        if model_name in ['LightGBM']:\n",
    "            estimator.set_params(scale_pos_weight=weight)\n",
    "        \n",
    "\n",
    "        estimator.fit(X_train_RFE, y_col)\n",
    "        fitted_estimators.append(estimator)\n",
    "\n",
    "    Y_pred = np.zeros_like(Y_test)\n",
    "    for i, estimator in enumerate(fitted_estimators):\n",
    "        Y_pred[:, i] = estimator.predict(X_test_RFE)\n",
    "    \n",
    "    model_configs[model_name]['predictions'] = Y_pred\n",
    "\n",
    "    subset_acc = accuracy_score(Y_test, Y_pred)\n",
    "    jaccard = jaccard_score(Y_test, Y_pred, average='samples', zero_division=0)\n",
    "    \n",
    "    results[model_name] = {'subset_accuracy': subset_acc, 'jaccard_score': jaccard}\n",
    "    \n",
    "    print(f\"[{model_name}] Subset Accuracy: {subset_acc:.4f}\")\n",
    "    print(f\"[{model_name}] Jaccard Score: {jaccard:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE MULTI-LABEL MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"{'Model':<20} | {'Subset Accuracy':<18} | {'Jaccard Score':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<20} | {metrics['subset_accuracy']:.4f}{'':<17} | {metrics['jaccard_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999d39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
