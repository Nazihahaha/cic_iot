{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7592210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_10880\\1938280762.py:18: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  'dns': pd.read_csv('labeled_dataset_DNS_Spoofing.csv'),\n",
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_10880\\1938280762.py:19: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  'mitm': pd.read_csv('labeled_dataset_MITM_ArpSpoofing.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(883525, 135)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"2\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"2\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load and combine datasets (as before)\n",
    "datasets = {\n",
    "    'benign': pd.read_csv('labeled_dataset_benign.csv'),\n",
    "    'dns': pd.read_csv('labeled_dataset_DNS_Spoofing.csv'),\n",
    "    'mitm': pd.read_csv('labeled_dataset_MITM_ArpSpoofing.csv')\n",
    "}\n",
    "combined_df = pd.concat([datasets['benign'], datasets['dns'], datasets['mitm']], ignore_index=True)\n",
    "combined_df = combined_df.drop('dst_mac', axis=1)\n",
    "\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89a1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "New shape after removing duplicates: (432555, 135)\n",
      "Categorical features: ['src_mac', 'src_ip', 'dst_ip', 'port_class_dst', 'l4_tcp', 'l4_udp', 'ttl', 'handshake_version', 'handshake_ciphersuites', 'tls_server', 'http_request_method', 'http_host', 'http_response_code', 'user_agent', 'dns_server', 'dns_query_type', 'dns_len_ans', 'device_mac', 'eth_src_oui', 'eth_dst_oui', 'highest_layer', 'http_uri', 'http_content_len', 'http_content_type', 'icmp_type', 'icmp_checksum_status', 'icmp_data_size', 'Label']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazih\\AppData\\Local\\Temp\\ipykernel_10880\\734139780.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  combined_df['All_Labels'] = combined_df.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def is_hex_mac(mac):\n",
    "    if pd.isna(mac) or mac == 'unknown':\n",
    "        return False\n",
    "    pattern = r'^([0-9A-Fa-f]{2}[:-]){5}([0-9A-Fa-f]{2})$'\n",
    "    return bool(re.match(pattern, str(mac)))\n",
    "\n",
    "\n",
    "combined_df = combined_df[~combined_df['src_mac'].apply(is_hex_mac)]\n",
    "\n",
    "# Drop duplicates\n",
    "print(f\"Number of duplicate rows: {combined_df.duplicated().sum()}\")\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "print(f\"New shape after removing duplicates: {combined_df.shape}\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = [col for col in combined_df.columns if combined_df[col].nunique() < 10 or combined_df[col].dtype == 'object']\n",
    "print(f\"Categorical features: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    combined_df[col] = combined_df[col].astype(str).fillna('unknown')\n",
    "\n",
    "numerical_cols = combined_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    combined_df[col] = combined_df[col].fillna(combined_df[col].median())\n",
    "\n",
    "X = combined_df.drop(['Label', 'src_mac'], axis=1, errors='ignore')\n",
    "y_attack = combined_df['Label']\n",
    "y_device = combined_df['src_mac']\n",
    "\n",
    "# Encode device labels\n",
    "le_device = LabelEncoder()\n",
    "y_device_encoded = le_device.fit_transform(y_device)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "combined_df['All_Labels'] = combined_df.apply(\n",
    "    lambda row: [row['Label'], row['src_mac']], axis=1\n",
    ")\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y_multilabel = mlb.fit_transform(combined_df['All_Labels'])\n",
    "\n",
    "multilabel_class_names = mlb.classes_\n",
    "\n",
    "\n",
    "X_numeric = X.select_dtypes(include=['number'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled,       \n",
    "    Y_multilabel,       \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4ca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RFE to select 50 features...\n",
      "Fitting estimator with 107 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:17:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 102 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:17:53] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 97 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 92 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 87 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 82 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 77 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:18:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 72 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 67 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 62 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:23] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 57 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 52 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:19:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE complete. Using 50 features.\n",
      "Selected features: [0, 1, 2, 3, 6]... (first 5 shown)\n",
      "\n",
      "======================================================================\n",
      "--- Training Multi-Label Random Forest (Binary Relevance) on RFE Features ---\n",
      "======================================================================\n",
      "Fitting individual Random Forest models (one per label)...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # New Model\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- RFE Setup and Execution (Same as before) ---\n",
    "\n",
    "# RFE Estimator: Use a fast XGBClassifier for feature ranking\n",
    "xgb_rfe_estimator = XGBClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=5, \n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "TARGET_FEATURES = 50 \n",
    "rfe_selector = RFE(\n",
    "    estimator=xgb_rfe_estimator, \n",
    "    n_features_to_select=TARGET_FEATURES, \n",
    "    step=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "Y_single_label = Y_train[:, 0]\n",
    "try:\n",
    "    multilabel_class_names = mlb.classes_\n",
    "except NameError:\n",
    "    multilabel_class_names = [f\"Label_{i+1}\" for i in range(Y_train.shape[1])]\n",
    "\n",
    "\n",
    "print(f\"Starting RFE to select {TARGET_FEATURES} features...\")\n",
    "rfe_selector.fit(X_train, Y_single_label)\n",
    "\n",
    "selected_features = X_train.columns[rfe_selector.support_]\n",
    "\n",
    "X_train_RFE = X_train[selected_features]\n",
    "X_test_RFE = X_test[selected_features]\n",
    "\n",
    "print(f\"RFE complete. Using {len(selected_features)} features.\")\n",
    "print(f\"Selected features: {selected_features.tolist()[:5]}... (first 5 shown)\")\n",
    "\n",
    "\n",
    "# --- Training Multi-Label Random Forest (Binary Relevance) on RFE Features ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Training Multi-Label Random Forest (Binary Relevance) on RFE Features ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Base Random Forest Estimator\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1, # Use all cores for fitting\n",
    "    class_weight='balanced' # Equivalent to scale_pos_weight in XGBoost\n",
    ")\n",
    "\n",
    "num_outputs = Y_train.shape[1]\n",
    "fitted_rf_estimators = [] \n",
    "\n",
    "print(\"Fitting individual Random Forest models (one per label)...\")\n",
    "\n",
    "for i in range(num_outputs):\n",
    "    y_col = Y_train[:, i]\n",
    "    \n",
    "    # We use class_weight='balanced' inside the base estimator, so no manual\n",
    "    # weight calculation (like scale_pos_weight) is strictly necessary here.\n",
    "    rf_estimator = clone(rf_base) \n",
    "    \n",
    "    # Train the estimator for the current label\n",
    "    rf_estimator.fit(X_train_RFE, y_col)\n",
    "    \n",
    "    fitted_rf_estimators.append(rf_estimator)\n",
    "\n",
    "# --- Prediction ---\n",
    "\n",
    "Y_pred_rf = np.zeros_like(Y_test)\n",
    "\n",
    "for i, estimator in enumerate(fitted_rf_estimators):\n",
    "    # Get the final prediction for the current label on the test set\n",
    "    Y_pred_rf[:, i] = estimator.predict(X_test_RFE)\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "subset_acc = accuracy_score(Y_test, Y_pred_rf)\n",
    "# Use 'samples' average for Jaccard, which is typically desired in multi-label classification\n",
    "jaccard = jaccard_score(Y_test, Y_pred_rf, average='samples', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RANDOM FOREST (RFE FEATURES) TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "print(f\"Jaccard Score (Label Similarity): {jaccard:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED MULTI-LABEL CLASSIFICATION REPORT (Per-Label Metrics)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "report = classification_report(\n",
    "    Y_test, \n",
    "    Y_pred_rf, \n",
    "    target_names=multilabel_class_names, \n",
    "    zero_division=0,\n",
    "    output_dict=False\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0db103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression # New Model\n",
    "from sklearn.svm import LinearSVC # New Model (for SVM with linear kernel)\n",
    "from sklearn.metrics import accuracy_score, jaccard_score, classification_report\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import catboost as cb # CatBoost Added\n",
    "import lightgbm as lgb # LightGBM Added\n",
    "\n",
    "# --- RFE Setup and Execution (Assuming it has been run successfully) ---\n",
    "# NOTE: The RFE selection is kept the same for a fair feature set.\n",
    "\n",
    "# --- Mock Data and RFE Run (for standalone execution) ---\n",
    "Y_single_label = Y_train[:, 0]\n",
    "try:\n",
    "    multilabel_class_names = mlb.classes_\n",
    "except NameError:\n",
    "    multilabel_class_names = [f\"Label_{i+1}\" for i in range(Y_train.shape[1])]\n",
    "\n",
    "\n",
    "TARGET_FEATURES = 50\n",
    "xgb_rfe_estimator = XGBClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1, use_label_encoder=False, eval_metric='logloss')\n",
    "rfe_selector = RFE(estimator=xgb_rfe_estimator, n_features_to_select=TARGET_FEATURES, step=5, verbose=0)\n",
    "\n",
    "# Run RFE\n",
    "print(f\"Starting RFE to select {TARGET_FEATURES} features...\")\n",
    "rfe_selector.fit(X_train, Y_single_label)\n",
    "\n",
    "selected_features = X_train.columns[rfe_selector.support_]\n",
    "X_train_RFE = X_train[selected_features]\n",
    "X_test_RFE = X_test[selected_features]\n",
    "\n",
    "print(f\"RFE complete. Using {len(selected_features)} features.\")\n",
    "\n",
    "model_configs = {\n",
    "    # Traditional Models\n",
    "    'RandomForest': {\n",
    "        'base_estimator': RandomForestClassifier(n_estimators=500, max_depth=10, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'base_estimator': LogisticRegression(solver='liblinear', random_state=42, n_jobs=1, class_weight='balanced'),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'LinearSVC': {\n",
    "        'base_estimator': LinearSVC(random_state=42, class_weight='balanced', dual=True),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        # AdaBoost often uses a shallow tree (DecisionTreeClassifier) as its base\n",
    "        'base_estimator': AdaBoostClassifier(n_estimators=500, random_state=42),\n",
    "        'predictions': None\n",
    "    },\n",
    "    # State-of-the-Art Boosting Models\n",
    "    'LightGBM': {\n",
    "        'base_estimator': lgb.LGBMClassifier(\n",
    "            n_estimators=500, \n",
    "            random_state=42, \n",
    "            n_jobs=-1,\n",
    "            # Handle imbalance via class weights\n",
    "            scale_pos_weight=1, # This will be set dynamically in the loop\n",
    "            objective='binary' \n",
    "        ),\n",
    "        'predictions': None\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'base_estimator': cb.CatBoostClassifier(\n",
    "            iterations=500, \n",
    "            random_state=42, \n",
    "            verbose=0, \n",
    "            thread_count=-1,\n",
    "            # Handle imbalance via class weights\n",
    "            auto_class_weights='Balanced', \n",
    "            loss_function='Logloss'\n",
    "        ),\n",
    "        'predictions': None\n",
    "    }\n",
    "}\n",
    "\n",
    "num_outputs = Y_train.shape[1]\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Starting Multi-Label Model Comparison (Binary Relevance) ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Training and Prediction Loop for all models ---\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    \n",
    "    base_estimator = config['base_estimator']\n",
    "    fitted_estimators = [] \n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        y_col = Y_train[:, i]\n",
    "        \n",
    "        # Calculate weight for highly imbalanced models (XGBoost, LightGBM)\n",
    "        pos_count = np.sum(y_col == 1)\n",
    "        neg_count = np.sum(y_col == 0)\n",
    "        weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "        estimator = clone(base_estimator) \n",
    "        \n",
    "        # Custom setting for models that use scale_pos_weight\n",
    "        if model_name in ['LightGBM']:\n",
    "            estimator.set_params(scale_pos_weight=weight)\n",
    "        \n",
    "        # Fit the model\n",
    "        # CatBoost requires its own data format if not using the default Scikit-learn wrapper\n",
    "        # We will use the standard wrapper for simplicity, as auto_class_weights is set.\n",
    "        estimator.fit(X_train_RFE, y_col)\n",
    "        fitted_estimators.append(estimator)\n",
    "\n",
    "    # Predict\n",
    "    Y_pred = np.zeros_like(Y_test)\n",
    "    for i, estimator in enumerate(fitted_estimators):\n",
    "        Y_pred[:, i] = estimator.predict(X_test_RFE)\n",
    "    \n",
    "    model_configs[model_name]['predictions'] = Y_pred\n",
    "\n",
    "    # Evaluate\n",
    "    subset_acc = accuracy_score(Y_test, Y_pred)\n",
    "    jaccard = jaccard_score(Y_test, Y_pred, average='samples', zero_division=0)\n",
    "    \n",
    "    results[model_name] = {'subset_accuracy': subset_acc, 'jaccard_score': jaccard}\n",
    "    \n",
    "    print(f\"[{model_name}] Subset Accuracy: {subset_acc:.4f}\")\n",
    "    print(f\"[{model_name}] Jaccard Score: {jaccard:.4f}\")\n",
    "\n",
    "# --- Final Summary and Detailed Report ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE MULTI-LABEL MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print Summary Table\n",
    "print(f\"{'Model':<20} | {'Subset Accuracy':<18} | {'Jaccard Score':<15}\")\n",
    "print(\"-\" * 70)\n",
    "# NOTE: You should manually insert your original XGBoost results here to complete the comparison!\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<20} | {metrics['subset_accuracy']:.4f}{'':<17} | {metrics['jaccard_score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999d39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
